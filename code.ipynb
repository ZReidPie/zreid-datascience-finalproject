{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setting Up and Using the AniList API**\n",
    "\n",
    "### **Tip 1**: Use a virtual environment to keep your dependencies isolated.  \n",
    "### **Tip 2**: Don’t worry about tokens expiring—they last a long time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Register Your Application on AniList**\n",
    "\n",
    "1. **Go to AniList Developer Settings**:  \n",
    "   - [AniList Developer Settings](https://anilist.co/settings/developer).\n",
    "   - [Heres the docs aswell](https://docs.anilist.co/guide/graphql/).\n",
    "\n",
    "2. **Create a New Application**:\n",
    "   - If you don’t have an AniList account, create one.\n",
    "   - **Application Name**: Choose a memorable name.\n",
    "   - **Redirect URL**: Use a dummy URL like `https://example.com/callback`.  \n",
    "     *(No web development is needed; this is just for OAuth).*\n",
    "\n",
    "3. **Save the Application**:\n",
    "   - You’ll receive:\n",
    "     - **Client ID**\n",
    "     - **Client Secret**\n",
    "   - **Important**: Save these securely.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Obtain an Authorization Code**\n",
    "\n",
    "### **Why Use Authorization Code Grant?**\n",
    "We’ll hide sensitive keys in a `.env` file and add it to `.gitignore` for security.\n",
    "\n",
    "### **Construct the Authorization URL**  \n",
    "Replace placeholders with your actual values:\n",
    "https://anilist.co/api/v2/oauth/authorize?client_id=YOUR_CLIENT_ID&response_type=code&redirect_uri=YOUR_REDIRECT_URI\n",
    "\n",
    "\n",
    "### 1. **Open the URL in a Browser**\n",
    "\n",
    "- Log in to AniList.\n",
    "- Grant permissions to your app.\n",
    "- You’ll be redirected to your specified redirect URL, which includes the AUTH_CODE as a query parameter (e.g., https://example.com/callback?code=AUTH_CODE).\n",
    "\n",
    "\n",
    "#### 2. **Copy the `AUTH_CODE` from the Redirect URL and save it to the `.env` file**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Exchange Authorization Code for Access Token**\n",
    "\n",
    "1. **Create a `.env` File**:  \n",
    "   Store your credentials securely in the following format:\n",
    "\n",
    "   CLIENT_ID=\"{INSERT}\"\n",
    "\n",
    "   CLIENT_SECRET=\"{INSERT}\"\n",
    "\n",
    "   REDIRECT_URI=\"{INSERT}\"\n",
    "   \n",
    "   AUTH_CODE=\"{INSERT}\"\n",
    "\n",
    "2. **Run the First Code Cell**:\n",
    "\n",
    "This makes a POST request to AniList's `token_url` and retrieves the `ACCESS_TOKEN`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the environment variables\n",
    "CLIENT_ID = os.getenv('CLIENT_ID')\n",
    "CLIENT_SECRET = os.getenv('CLIENT_SECRET')\n",
    "REDIRECT_URI = os.getenv('REDIRECT_URI')\n",
    "ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')\n",
    "AUTH_CODE = os.getenv('AUTH_CODE')\n",
    "\n",
    "# Token request payload\n",
    "token_url = 'https://anilist.co/api/v2/oauth/token' # public endpoint for Anilist \n",
    "payload = {\n",
    "    'grant_type': 'authorization_code',\n",
    "    'client_id': CLIENT_ID,\n",
    "    'client_secret': CLIENT_SECRET,\n",
    "    'redirect_uri': REDIRECT_URI,\n",
    "    'code': AUTH_CODE\n",
    "}\n",
    "\n",
    "# Send POST request to exchange code for access token\n",
    "response = requests.post(token_url, data=payload)\n",
    "token_data = response.json()\n",
    "\n",
    "# Print the access token\n",
    "\n",
    "# Uncomment this, doesn't work after a couple of tries to mark down result\n",
    "#print(\"Access Token:\", token_data['access_token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Add the ACCESS_TOKEN to Your .env File:**\n",
    "    \n",
    "    ACCESS_TOKEN=\"{INSERT}\"\n",
    "\n",
    "4. **Testing json data**\n",
    "\n",
    "    Now run the cell below to test if you can fetch the anime json data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'Media': {'id': 1, 'title': {'romaji': 'Cowboy Bebop', 'english': 'Cowboy Bebop', 'native': 'カウボーイビバップ'}, 'format': 'TV', 'episodes': 26, 'status': 'FINISHED', 'averageScore': 86, 'popularity': 372252, 'genres': ['Action', 'Adventure', 'Drama', 'Sci-Fi']}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Your access token\n",
    "ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')\n",
    "\n",
    "# Authorization headers\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {ACCESS_TOKEN}'\n",
    "}\n",
    "\n",
    "# GraphQL query to fetch anime details\n",
    "query = '''\n",
    "query ($id: Int) {\n",
    "  Media(id: $id, type: ANIME) {\n",
    "    id\n",
    "    title {\n",
    "      romaji\n",
    "      english\n",
    "      native\n",
    "    }\n",
    "    format\n",
    "    episodes\n",
    "    status\n",
    "    averageScore\n",
    "    popularity\n",
    "    genres\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Variables for the query\n",
    "variables = {\n",
    "    'id': 1  # Example ID for \"Cowboy Bebop\"\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "url = 'https://graphql.anilist.co'\n",
    "response = requests.post(url, json={'query': query, 'variables': variables}, headers=headers)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for the last response should be \n",
    "\n",
    "{'data': {'Media': {'id': 1, 'title': {'romaji': 'Cowboy Bebop', 'english': 'Cowboy Bebop', 'native': 'カウボーイビバップ'}, 'format': 'TV', 'episodes': 26, 'status': 'FINISHED', 'averageScore': 86, 'popularity': 372155, 'genres': ['Action', 'Adventure', 'Drama', 'Sci-Fi']}}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to train a machine learning model using data collected from the AniList API to predict an anime's true rating based on a variety of factors. The features used in the training process include attributes such as studio, number of episodes, season, popularity, favorites, and other relevant metrics.\n",
    "\n",
    "At the end of the project, the model should be able to take a valid anime title as input, calculate its predicted \"true rating,\" and compare the result to the original rating listed on AniList. This approach aims to provide insights into how various factors contribute to an anime's rating and evaluate the model's predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit. Waiting for 60 seconds...\n",
      "Total anime fetched: 2000\n",
      "Fetched 2000 TV anime and saved to 'data/raw_anilist.csv'\n"
     ]
    }
   ],
   "source": [
    "# Anilist Fetching animes and making CSV file \n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Your AniList access token\n",
    "ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')\n",
    "\n",
    "# Authorization headers\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {ACCESS_TOKEN}'\n",
    "}\n",
    "\n",
    "# GraphQL query to fetch anime details (only TV format)\n",
    "query = '''\n",
    "query ($page: Int, $perPage: Int) {\n",
    "  Page(page: $page, perPage: $perPage) {\n",
    "    media(type: ANIME, format: TV) {\n",
    "      id\n",
    "      title {\n",
    "        romaji\n",
    "        english\n",
    "      }\n",
    "      episodes\n",
    "      status\n",
    "      averageScore\n",
    "      popularity\n",
    "      favourites\n",
    "      genres\n",
    "      studios {\n",
    "        edges {\n",
    "          node {\n",
    "            name\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "      source\n",
    "      season\n",
    "      startDate {\n",
    "        year\n",
    "        month\n",
    "        day\n",
    "      }\n",
    "      endDate {\n",
    "        year\n",
    "        month\n",
    "        day\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Anilist Fetching animes and making CSV file\n",
    "def fetch_anime_details(total_anime=500, animes_per_page=50):\n",
    "    anime_list = []\n",
    "    current_page = 1\n",
    "\n",
    "    while len(anime_list) < total_anime:\n",
    "        # Variables for the query\n",
    "        variables = {\n",
    "            'page': current_page,\n",
    "            'perPage': animes_per_page\n",
    "        }\n",
    "\n",
    "        # Make the request\n",
    "        url = 'https://graphql.anilist.co'\n",
    "        response = requests.post(url, json={'query': query, 'variables': variables}, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            fetched_anime = data['data']['Page']['media']\n",
    "            anime_list.extend(fetched_anime)\n",
    "            \n",
    "            # Stop if there are no more anime to fetch\n",
    "            if len(fetched_anime) < animes_per_page:\n",
    "                print(\"Reached the end of available anime.\")\n",
    "                break\n",
    "        elif response.status_code == 429:  # Too Many Requests Error\n",
    "            print(f\"Rate limit hit. Waiting for 60 seconds...\")\n",
    "            time.sleep(60)  # Wait longer when rate-limited\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Error fetching page {current_page}: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "\n",
    "        # Delay to respect API limits\n",
    "        time.sleep(1)\n",
    "        current_page += 1\n",
    "\n",
    "    # Trim the list to the requested total_anime count\n",
    "    anime_list = anime_list[:total_anime]\n",
    "\n",
    "    print(f\"Total anime fetched: {len(anime_list)}\")\n",
    "    return anime_list\n",
    "\n",
    "# Fetch anime details\n",
    "total_anime_to_fetch = 2000 # How many animes you want\n",
    "animes_per_page = 50 \n",
    "# Higher animes_per_page: \n",
    "# A single request retrieves more data, reducing the total number of API calls. \n",
    "# However, larger payloads take longer to process on both the API server and your client.\n",
    "anime_data = fetch_anime_details(total_anime=total_anime_to_fetch, animes_per_page=animes_per_page)\n",
    "\n",
    "# Process the data into a DataFrame\n",
    "processed_anime_data = []\n",
    "\n",
    "for anime in anime_data:\n",
    "    studios = \", \".join([studio['node']['name'] for studio in anime['studios']['edges']])\n",
    "    start_date = f\"{anime['startDate']['year']}-{anime['startDate']['month']}-{anime['startDate']['day']}\" if anime['startDate'] else None\n",
    "    end_date = f\"{anime['endDate']['year']}-{anime['endDate']['month']}-{anime['endDate']['day']}\" if anime['endDate'] else None\n",
    "    processed_anime_data.append({\n",
    "        'id': anime['id'],\n",
    "        'title_romaji': anime['title']['romaji'],\n",
    "        'title_english': anime['title']['english'],\n",
    "        'episodes': anime['episodes'],\n",
    "        'status': anime['status'],\n",
    "        'average_score': anime['averageScore'],\n",
    "        'popularity': anime['popularity'],\n",
    "        'favourites': anime['favourites'],\n",
    "        'genres': \", \".join(anime['genres']),\n",
    "        'studios': studios,\n",
    "        'source': anime['source'],\n",
    "        'season': anime['season'],\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(processed_anime_data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('data/raw_anilist.csv', index=False)\n",
    "print(f\"Fetched {total_anime_to_fetch} TV anime and saved to 'data/raw_anilist.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                title_romaji           title_english  episodes  \\\n",
      "0    1                Cowboy Bebop            Cowboy Bebop      26.0   \n",
      "1    6                      TRIGUN                  Trigun      26.0   \n",
      "2    7          Witch Hunter ROBIN      Witch Hunter ROBIN      26.0   \n",
      "3    8              Bouken Ou Beet  Beet the Vandel Buster      52.0   \n",
      "4   15                Eyeshield 21            Eyeshield 21     145.0   \n",
      "5   16        Hachimitsu to Clover        Honey and Clover      24.0   \n",
      "6   17  Hungry Heart: Wild Striker                     NaN      52.0   \n",
      "7   18      Initial D FOURTH STAGE     Initial D 4th Stage      24.0   \n",
      "8   19                     MONSTER                 Monster      74.0   \n",
      "9   20                      NARUTO                  Naruto     220.0   \n",
      "10  21                   ONE PIECE               ONE PIECE       NaN   \n",
      "11  22         Tennis no Ouji-sama    The Prince of Tennis     178.0   \n",
      "12  23            Ring ni Kakero 1                     NaN      12.0   \n",
      "13  24               School Rumble           School Rumble      26.0   \n",
      "14  25                   Sunabouzu             Desert Punk      24.0   \n",
      "\n",
      "       status  average_score  popularity  favourites  \\\n",
      "0    FINISHED           86.0      372252       24316   \n",
      "1    FINISHED           80.0      133477        5274   \n",
      "2    FINISHED           68.0       18157         204   \n",
      "3    FINISHED           65.0        2555          29   \n",
      "4    FINISHED           76.0       28520         662   \n",
      "5    FINISHED           76.0       49595         816   \n",
      "6    FINISHED           71.0        4066          71   \n",
      "7    FINISHED           80.0       39032         770   \n",
      "8    FINISHED           88.0      249214       17502   \n",
      "9    FINISHED           79.0      571389       27314   \n",
      "10  RELEASING           88.0      567668       82301   \n",
      "11   FINISHED           75.0       30164         623   \n",
      "12   FINISHED           60.0        1381          18   \n",
      "13   FINISHED           76.0       49966         975   \n",
      "14   FINISHED           68.0       23793         304   \n",
      "\n",
      "                                               genres  \\\n",
      "0                    Action, Adventure, Drama, Sci-Fi   \n",
      "1            Action, Adventure, Comedy, Drama, Sci-Fi   \n",
      "2                Action, Drama, Mystery, Supernatural   \n",
      "3                    Adventure, Fantasy, Supernatural   \n",
      "4                              Action, Comedy, Sports   \n",
      "5               Comedy, Drama, Romance, Slice of Life   \n",
      "6                       Comedy, Slice of Life, Sports   \n",
      "7                               Action, Drama, Sports   \n",
      "8     Drama, Horror, Mystery, Psychological, Thriller   \n",
      "9   Action, Adventure, Comedy, Drama, Fantasy, Sup...   \n",
      "10          Action, Adventure, Comedy, Drama, Fantasy   \n",
      "11                             Action, Comedy, Sports   \n",
      "12                                     Action, Sports   \n",
      "13                     Comedy, Romance, Slice of Life   \n",
      "14           Action, Adventure, Comedy, Ecchi, Sci-Fi   \n",
      "\n",
      "                                              studios    source  season  \\\n",
      "0        Sunrise, Bandai Visual, Bandai Entertainment  ORIGINAL  SPRING   \n",
      "1       MADHOUSE, Nippon Victor, Arts Pro, Funimation     MANGA  SPRING   \n",
      "2        Sunrise, Bandai Visual, Bandai Entertainment  ORIGINAL  SUMMER   \n",
      "3                    Toei Animation, Dentsu, TV Tokyo     MANGA    FALL   \n",
      "4   Studio Gallop, NAS, Nihon Ad Systems, Sentai F...     MANGA  SPRING   \n",
      "5   J.C. Staff, Nomad, Genco, Viz Media, Asmik Ace...     MANGA  SPRING   \n",
      "6                           Nippon Animation, Fuji TV     MANGA    FALL   \n",
      "7      OB Planning, Funimation, A.C.G.T., Studio Jack     MANGA  SPRING   \n",
      "8   MADHOUSE, VAP, Viz Media, Nippon Television Ne...     MANGA  SPRING   \n",
      "9   Studio Pierrot, TV Tokyo, Aniplex, Viz Media, ...     MANGA    FALL   \n",
      "10  Toei Animation, Funimation, Fuji TV, 4Kids Ent...     MANGA    FALL   \n",
      "11  Production I.G, Trans Arts, Viz Media, NAS, Fu...     MANGA    FALL   \n",
      "12            Toei Animation, Marvelous Entertainment     MANGA    FALL   \n",
      "13  TV Tokyo, Sotsu, Marvelous Entertainment, Star...     MANGA    FALL   \n",
      "14           GONZO, GDH, Funimation, Pony Canyon, CBC     MANGA    FALL   \n",
      "\n",
      "    start_date        end_date  \n",
      "0     1998-4-3       1999-4-24  \n",
      "1     1998-4-1       1998-9-30  \n",
      "2     2002-7-2      2002-12-24  \n",
      "3    2004-9-30       2005-9-29  \n",
      "4     2005-4-6       2008-3-19  \n",
      "5    2005-4-15       2005-9-27  \n",
      "6    2002-9-11       2003-9-10  \n",
      "7    2004-4-17       2006-2-18  \n",
      "8     2004-4-7       2005-9-28  \n",
      "9    2002-10-3        2007-2-8  \n",
      "10  1999-10-20  None-None-None  \n",
      "11  2001-10-10       2005-3-23  \n",
      "12   2004-10-6      2004-12-15  \n",
      "13   2004-10-5       2005-3-29  \n",
      "14   2004-10-6       2005-3-30  \n"
     ]
    }
   ],
   "source": [
    "# lets view our rows to make sure everything is there \n",
    "print(pd.read_csv(file_path, nrows=15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed. 'train.csv' and 'test.csv' have been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "# Specify the file path for the dataset containing anime details\n",
    "file_path = 'data/raw_anilist.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "# Use an 80-20 split: 80% of the data will be used for training, 20% for testing\n",
    "# random_state ensures reproducibility of the split\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Save the splits to separate CSV files\n",
    "# Save the training data to 'train.csv' for model training\n",
    "train_data.to_csv('data/train.csv', index=False) # Ensure the index is not included in the saved file\n",
    "# Save the testing data to 'test.csv' for model evaluation\n",
    "test_data.to_csv('data/test.csv', index=False)\n",
    "\n",
    "# Step 4: Confirmation message\n",
    "print(\"Data split completed. 'train.csv' and 'test.csv' have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Import the pandas library for data manipulation and analysis\n",
    "# Step 1: Define file paths for training and testing data\n",
    "# Specify the file paths for the training and testing datasets\n",
    "train_path = 'data/train.csv'\n",
    "test_path = 'data/test.csv'\n",
    "\n",
    "# Step 2: Load the data into DataFrames\n",
    "# Use pandas to read the CSV files and load them into DataFrames\n",
    "train_df = pd.read_csv(train_path) # Load the training dataset into 'train_df'\n",
    "test_df = pd.read_csv(test_path) # Load the testing dataset into 'test_df'\n",
    "\n",
    "# At this point, 'train_df' contains the data for training the model,\n",
    "# and 'test_df' contains the data for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def preprocess_and_engineer_features(df):\n",
    "    # Convert start and end dates to datetime\n",
    "    df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "    df['end_date'] = pd.to_datetime(df['end_date'], errors='coerce')\n",
    "\n",
    "    # Extract year from start date\n",
    "    df['start_year'] = df['start_date'].dt.year.fillna(0).astype(int)\n",
    "    df['end_year'] = df['end_date'].dt.year.fillna(0).astype(int)\n",
    "\n",
    "    # Encode season numerically (e.g., SPRING = 1, SUMMER = 2, etc.)\n",
    "    season_mapping = {'WINTER': 1, 'SPRING': 2, 'SUMMER': 3, 'FALL': 4}\n",
    "    df['season_encoded'] = df['season'].map(season_mapping).fillna(0).astype(int)\n",
    "\n",
    "    # Calculate anime runtime in days\n",
    "    df['runtime_days'] = (df['end_date'] - df['start_date']).dt.days.fillna(0).astype(int)\n",
    "\n",
    "    # Log-transform numerical features to reduce skewness\n",
    "    df['log_popularity'] = np.log1p(df['popularity'])\n",
    "    df['log_favorites'] = np.log1p(df['favourites'])\n",
    "\n",
    "    # One-hot encode genres (can handle multiple genres per anime)\n",
    "    df['genres'] = df['genres'].str.split(', ')  # Split genres into lists\n",
    "    genre_dummies = df['genres'].explode().str.get_dummies().groupby(level=0).sum()\n",
    "    df = pd.concat([df, genre_dummies], axis=1)\n",
    "\n",
    "    # Studios encoding (use a frequency encoding)\n",
    "    studio_counts = df['studios'].value_counts().to_dict()\n",
    "    df['studio_frequency'] = df['studios'].map(studio_counts).fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess datasets\n",
    "train_df = preprocess_and_engineer_features(train_df)\n",
    "test_df = preprocess_and_engineer_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_romaji</th>\n",
       "      <th>title_english</th>\n",
       "      <th>episodes</th>\n",
       "      <th>status</th>\n",
       "      <th>average_score</th>\n",
       "      <th>popularity</th>\n",
       "      <th>favourites</th>\n",
       "      <th>genres</th>\n",
       "      <th>studios</th>\n",
       "      <th>...</th>\n",
       "      <th>Music</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Psychological</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Slice of Life</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Supernatural</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>studio_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2562</td>\n",
       "      <td>Shion no Ou: The Flowers of Hard Blood</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>71.0</td>\n",
       "      <td>5920</td>\n",
       "      <td>39</td>\n",
       "      <td>[Drama, Mystery, Thriller]</td>\n",
       "      <td>Studio DEEN, Yomiko Advertising, Pony Canyon, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                            title_romaji title_english  episodes  \\\n",
       "0  2562  Shion no Ou: The Flowers of Hard Blood           NaN      22.0   \n",
       "\n",
       "     status  average_score  popularity  favourites  \\\n",
       "0  FINISHED           71.0        5920          39   \n",
       "\n",
       "                       genres  \\\n",
       "0  [Drama, Mystery, Thriller]   \n",
       "\n",
       "                                             studios  ... Music Mystery  \\\n",
       "0  Studio DEEN, Yomiko Advertising, Pony Canyon, ...  ...     0       1   \n",
       "\n",
       "  Psychological Romance  Sci-Fi  Slice of Life  Sports  Supernatural  \\\n",
       "0             0       0       0              0       0             0   \n",
       "\n",
       "   Thriller  studio_frequency  \n",
       "0         1               1.0  \n",
       "\n",
       "[1 rows x 39 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_romaji</th>\n",
       "      <th>title_english</th>\n",
       "      <th>episodes</th>\n",
       "      <th>status</th>\n",
       "      <th>average_score</th>\n",
       "      <th>popularity</th>\n",
       "      <th>favourites</th>\n",
       "      <th>genres</th>\n",
       "      <th>studios</th>\n",
       "      <th>...</th>\n",
       "      <th>Music</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Psychological</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Slice of Life</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Supernatural</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>studio_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9314</td>\n",
       "      <td>Fractale</td>\n",
       "      <td>Fractale</td>\n",
       "      <td>11.0</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>65.0</td>\n",
       "      <td>15065</td>\n",
       "      <td>80</td>\n",
       "      <td>[Adventure, Fantasy, Sci-Fi]</td>\n",
       "      <td>A-1 Pictures, Funimation, Ordet, Asmik Ace, So...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id title_romaji title_english  episodes    status  average_score  \\\n",
       "0  9314     Fractale      Fractale      11.0  FINISHED           65.0   \n",
       "\n",
       "   popularity  favourites                        genres  \\\n",
       "0       15065          80  [Adventure, Fantasy, Sci-Fi]   \n",
       "\n",
       "                                             studios  ... Music Mystery  \\\n",
       "0  A-1 Pictures, Funimation, Ordet, Asmik Ace, So...  ...     0       0   \n",
       "\n",
       "  Psychological Romance  Sci-Fi  Slice of Life  Sports  Supernatural  \\\n",
       "0             0       0       1              0       0             0   \n",
       "\n",
       "   Thriller  studio_frequency  \n",
       "0         0               1.0  \n",
       "\n",
       "[1 rows x 39 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[03:15:31] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\data\\data.cc:514: Check failed: valid: Label contains NaN, infinity or a value too large.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[277], line 63\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Train the model (XGBoost Regressor)\u001b[39;00m\n\u001b[0;32m     57\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBRegressor(\n\u001b[0;32m     58\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     59\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     60\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[0;32m     61\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     66\u001b[0m y_val_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\sklearn.py:1081\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[0;32m   1080\u001b[0m     evals_result: TrainingCallback\u001b[38;5;241m.\u001b[39mEvalsLog \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1081\u001b[0m     train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\sklearn.py:596\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[0;32m    577\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m    578\u001b[0m     X: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    592\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 596\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\sklearn.py:1003\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1003\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bin\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:1573\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1554\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m         )\n\u001b[0;32m   1567\u001b[0m     ):\n\u001b[0;32m   1568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1571\u001b[0m         )\n\u001b[1;32m-> 1573\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:1632\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[1;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[0;32m   1620\u001b[0m config \u001b[38;5;241m=\u001b[39m make_jcargs(\n\u001b[0;32m   1621\u001b[0m     nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnthread, missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing, max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin\n\u001b[0;32m   1622\u001b[0m )\n\u001b[0;32m   1623\u001b[0m ret \u001b[38;5;241m=\u001b[39m _LIB\u001b[38;5;241m.\u001b[39mXGQuantileDMatrixCreateFromCallback(\n\u001b[0;32m   1624\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1625\u001b[0m     it\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1630\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(handle),\n\u001b[0;32m   1631\u001b[0m )\n\u001b[1;32m-> 1632\u001b[0m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m _check_call(ret)\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:569\u001b[0m, in \u001b[0;36mDataIter.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    567\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:550\u001b[0m, in \u001b[0;36mDataIter._handle_exception\u001b[1;34m(self, fn, dft_ret)\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[0;32m    555\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:637\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\data.py:1402\u001b[0m, in \u001b[0;36mSingleBatchInternalIter.next\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m   1400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1402\u001b[0m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:626\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.input_data\u001b[1;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m (new, cat_codes, feature_names, feature_types)\n\u001b[0;32m    625\u001b[0m dispatch_proxy_set_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy, new, cat_codes)\n\u001b[1;32m--> 626\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_ref \u001b[38;5;241m=\u001b[39m ref\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:954\u001b[0m, in \u001b[0;36mDMatrix.set_info\u001b[1;34m(self, label, weight, base_margin, group, qid, label_lower_bound, label_upper_bound, feature_names, feature_types, feature_weights)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 954\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_weight(weight)\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:1092\u001b[0m, in \u001b[0;36mDMatrix.set_label\u001b[1;34m(self, label)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set label of dmatrix\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \n\u001b[0;32m   1085\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;124;03m    The label information to be set into DMatrix\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[1;32m-> 1092\u001b[0m \u001b[43mdispatch_meta_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\data.py:1348\u001b[0m, in \u001b[0;36mdispatch_meta_backend\u001b[1;34m(matrix, data, name, dtype)\u001b[0m\n\u001b[0;32m   1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_series(data):\n\u001b[1;32m-> 1348\u001b[0m     \u001b[43m_meta_from_pandas_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_dlpack(data):\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\data.py:679\u001b[0m, in \u001b[0;36m_meta_from_pandas_series\u001b[1;34m(data, name, dtype, handle)\u001b[0m\n\u001b[0;32m    677\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_dense()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 679\u001b[0m \u001b[43m_meta_from_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\data.py:1279\u001b[0m, in \u001b[0;36m_meta_from_numpy\u001b[1;34m(data, field, dtype, handle)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMasked array is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1278\u001b[0m interface_str \u001b[38;5;241m=\u001b[39m _array_interface(data)\n\u001b[1;32m-> 1279\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGDMatrixSetInfoFromInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zande\\OneDrive\\Desktop\\College Classes\\Fall 2024\\CS 506\\Final Project\\zreid-datascience-finalproject\\env\\Lib\\site-packages\\xgboost\\core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [03:15:31] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\data\\data.cc:514: Check failed: valid: Label contains NaN, infinity or a value too large."
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Define target and features\n",
    "target = 'average_score'\n",
    "features = train_df.drop(columns=['id', 'start_date', 'end_date', target])  # Drop unnecessary columns\n",
    "test_features = test_df.drop(columns=['id', 'start_date', 'end_date', target])\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = features.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = features.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessors\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessors\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess datasets\n",
    "X_train_full = features\n",
    "X_test_full = test_features\n",
    "y_train_full = train_df[target]\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess features\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_val = preprocessor.transform(X_val)\n",
    "X_test = preprocessor.transform(X_test_full)\n",
    "\n",
    "# Train the model (XGBoost Regressor)\n",
    "model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=20,\n",
    "    learning_rate=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "# Predict on the test dataset\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Create a submission file\n",
    "submission = test_df.copy()\n",
    "submission['true_rating'] = test_predictions\n",
    "submission[['title_romaji', 'average_score', 'true_rating']].to_csv('data/submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            title_romaji  average_score  true_rating\n",
      "0          School Rumble             76    78.002860\n",
      "1              Black Cat             69    72.009840\n",
      "2                Chobits             70    77.049540\n",
      "3      Soukyuu no Fafner             69    60.284450\n",
      "4       Yakitate!! Japan             75    70.415060\n",
      "5               SHUFFLE!             65    68.347466\n",
      "6            Arc the Lad             61    57.632355\n",
      "7          Ai Yori Aoshi             67    65.878880\n",
      "8              D.N.Angel             66    67.905426\n",
      "9  Shin Seiki Evangelion             83    87.948590\n",
      "\n",
      "Validation MAE: 3.0174\n",
      "Validation RMSE: 3.6896\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.read_csv('data/submission.csv')\n",
    "# Replace 'column_name' with the actual column you want to view\n",
    "print(submission_df.head(10))\n",
    "print(\"\")\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
